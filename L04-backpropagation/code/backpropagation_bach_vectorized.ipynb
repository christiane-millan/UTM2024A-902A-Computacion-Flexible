{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento Backpropagation batch - step to step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de la red\n",
    "\n",
    "<div>\n",
    "<img src=\"../img/rn_online_2-2-2.png\" align=\"center\" width=\"400\"/>\n",
    "<div style=\"text-align: justify;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = np.array([[0.15, 0.25],\n",
    "               [0.20, 0.30]])\n",
    "bh = np.array([0.35, 0.35])\n",
    "\n",
    "w_out = np.array([[0.40, 0.50],\n",
    "                  [0.45, 0.55]])\n",
    "b_out = np.array([0.60, 0.60])\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [1, 0],\n",
    "              [0, 1],\n",
    "              [1, 1]])\n",
    "y = np.array([[0, 1],\n",
    "              [1, 0],\n",
    "              [1, 0],\n",
    "              [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35 0.35]\n",
      " [0.5  0.6 ]\n",
      " [0.55 0.65]\n",
      " [0.7  0.9 ]]\n"
     ]
    }
   ],
   "source": [
    "z_h = np.dot(X, wh) + bh\n",
    "print(z_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmod (z):\n",
    "  return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58661758 0.58661758]\n",
      " [0.62245933 0.64565631]\n",
      " [0.63413559 0.65701046]\n",
      " [0.66818777 0.7109495 ]]\n"
     ]
    }
   ],
   "source": [
    "a_h = sigmod(z_h)\n",
    "print(a_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.09862494 1.21594846]\n",
      " [1.13952907 1.26634063]\n",
      " [1.14930894 1.27842355]\n",
      " [1.18720239 1.32511611]]\n"
     ]
    }
   ],
   "source": [
    "z_out = np.dot(a_h, w_out) + b_out\n",
    "print(z_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75000237 0.77134977]\n",
      " [0.75759317 0.78011568]\n",
      " [0.75938467 0.78218131]\n",
      " [0.76624034 0.79003164]]\n"
     ]
    }
   ],
   "source": [
    "a_out = sigmod(z_out)\n",
    "print(a_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "## Ajuste de los pesos de la capa de salida\n",
    "\n",
    "Consideremos un peso en particular de la capa de salida\n",
    "\n",
    "$\\Delta w_{h_j, o_k} \\alpha - \\frac{\\partial E}{\\partial w_{h_j,o_k}}$\n",
    "\n",
    "Es necesario considerar que el error no es directamente una función de un error. Se expande como:\n",
    "\n",
    "$\\Delta w_{h_j,o_kj} = - \\eta\n",
    "\\frac{\\partial E}{\\partial out_{o_k}} * \n",
    "\\frac{\\partial out_{o_k}}{\\partial net_{o_k}} * \n",
    "\\frac{\\partial net_{o_k}}{\\partial w_{h_j,h_k}}$\n",
    "\n",
    "### Regla de cambio de pesos para un peso entre la capa oculta y de salida\n",
    "\n",
    "$\\Delta w_{h_j,o_k} = - \\eta  (t_{o_k} - a_k^{(out)}) * a_k^{(out)} * ( 1 - a_k^{(out)}) * a_j^{(h)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derivada del error con respecto a la activación: $- (t_{k} - a_k^{(out)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75000237  0.22865023]\n",
      " [ 0.24240683 -0.78011568]\n",
      " [ 0.24061533 -0.78218131]\n",
      " [-0.76624034  0.20996836]]\n"
     ]
    }
   ],
   "source": [
    "delta_out = y - a_out\n",
    "print(delta_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Derivada de la activación con respecto a la entrada: $a_k^{(out)} * ( 1 - a_k^{(out)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18749881 0.1763693 ]\n",
      " [0.18364576 0.17153521]\n",
      " [0.18271959 0.17037371]\n",
      " [0.17911608 0.16588165]]\n"
     ]
    }
   ],
   "source": [
    "sigmod_derivative = a_out * (1 - a_out)\n",
    "print(sigmod_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Derivada de la entrada con respecto al peso: $a_j^{(h)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58661758 0.58661758]\n",
      " [0.62245933 0.64565631]\n",
      " [0.63413559 0.65701046]\n",
      " [0.66818777 0.7109495 ]]\n"
     ]
    }
   ],
   "source": [
    "print(a_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se considera que $\\delta_k^{(out)} = (t_k - a_k^{(out)}) * a_k^{(out)} * (1- a_k^{(out)}) $￼, entonces la regla es muy similar a la del Perceptrón.\n",
    "\n",
    "$\\Delta w_{h_j, o_k} = - \\eta \\delta_{o_k} * a_j^{(h)}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14062456  0.04032688]\n",
      " [ 0.04451699 -0.1338173 ]\n",
      " [ 0.04396514 -0.13326313]\n",
      " [-0.13724597  0.0348299 ]]\n"
     ]
    }
   ],
   "source": [
    "delta = delta_out * sigmod_derivative\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de pesos de la capa oculta\n",
    "\n",
    "$\\Delta w_{ji} \\, \\alpha -\n",
    "\\big[ \n",
    "\\sum_k \n",
    "\\frac{\\partial E_{total}}{\\partial out_{o_k}} \\frac{\\partial out_{o_k}}{\\partial net_{o_k}} \\frac{\\partial net_{o_k}}{\\partial out_{h_j}}\n",
    "\\big] \n",
    "\\frac{\\partial out_{h_j}}{\\partial net_{h_j}} \n",
    "\\frac{\\partial net_{h_j}}{\\partial w_{h_j,i_i}}$\n",
    "\n",
    "$\\Delta w_{ji} = \\eta \n",
    "\\big[ \n",
    "\\sum_k \n",
    "(t_{o_k} - a_k^{(out)}) * a_k^{(out)} * (1- a_k^{(out)}) w_{h_j, o_k}\n",
    "\\big] \n",
    "a_j^{(h)} (1 - a_j^{(h)})\n",
    "a_i^{(in)}$\n",
    "\n",
    "$\\Delta w_{ji} = \\eta \n",
    "\\big[ \n",
    "\\sum_k \n",
    "\\delta_k^{(out)} w_{h_j, o_k}\n",
    "\\big] \n",
    "a_j^{(h)} (1 - a_j^{(h)})\n",
    "a_i^{(in)}$\n",
    "￼￼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03608638 -0.04110126]\n",
      " [-0.04910186 -0.05356687]\n",
      " [-0.04904551 -0.05351041]\n",
      " [-0.03748344 -0.04260424]]\n"
     ]
    }
   ],
   "source": [
    "delta_h = np.dot(delta ,w_out.T)\n",
    "print(delta_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2424974  0.2424974 ]\n",
      " [0.23500371 0.22878424]\n",
      " [0.23200764 0.22534771]\n",
      " [0.22171287 0.20550031]]\n"
     ]
    }
   ],
   "source": [
    "sigmod_derivative_h = a_h * (1 - a_h)\n",
    "print(sigmod_derivative_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00875085 -0.00996695]\n",
      " [-0.01153912 -0.01225526]\n",
      " [-0.01137893 -0.01205845]\n",
      " [-0.00831056 -0.00875518]]\n"
     ]
    }
   ],
   "source": [
    "delta_h = delta_h * sigmod_derivative_h\n",
    "print(delta_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta para la capa oculta\n",
    "\n",
    "\n",
    "## Regla de cambio de pesos entre la capa de entrada y oculta\n",
    "\n",
    "Si consideramos que $\\delta_{h_j} = \\big[ \\sum_k w_{o_k,h_j} \\big ] * out_{h_j} * (1- out_{h_j}) $, entonces la regla es muy similar a la del Perceptrón.\n",
    "\n",
    "$\\Delta w_{ij} = - \\eta \\delta_{h_j} out_{i_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01984968 -0.01968949]\n",
      " [-0.02101044 -0.02081363]]\n",
      "\n",
      "Gradiente de b h\n",
      " [-0.03997947 -0.04303584]\n"
     ]
    }
   ],
   "source": [
    "grad_w_h = np.dot(delta_h.T, X)\n",
    "grad_b_h = np.sum(delta_h, axis = 0)\n",
    "print(grad_w_h)\n",
    "print('\\nGradiente de b h\\n', grad_b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta para la capa de salida\n",
    "\n",
    "### Regla de cambio de pesos para un peso entre la capa oculta y de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11860904 -0.12243956]\n",
      " [-0.12087335 -0.1255365 ]]\n",
      "\n",
      "Gradiente de b out\n",
      " [-0.1893884  -0.19192365]\n"
     ]
    }
   ],
   "source": [
    "grad_w_out = np.dot(delta.T, a_h)\n",
    "grad_b_out = np.sum(delta, axis = 0)\n",
    "print(grad_w_out)\n",
    "print('\\nGradiente de b out\\n', grad_b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambio de pesos para un peso entre la capa entrada y oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15992484 0.25984475]\n",
      " [0.21050522 0.31040682]]\n",
      "\n",
      "Bias\n",
      " [0.36998973 0.37151792]\n"
     ]
    }
   ],
   "source": [
    "wh -= 0.5 * grad_w_h\n",
    "bh -= 0.5 * grad_b_h\n",
    "\n",
    "print(wh)\n",
    "print('\\nBias\\n', bh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste de pesos de capa de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45930452 0.56121978]\n",
      " [0.51043668 0.61276825]]\n",
      "\n",
      "Bias\n",
      " [0.6946942  0.69596183]\n"
     ]
    }
   ],
   "source": [
    "w_out -= 0.5 * grad_w_out\n",
    "b_out -= 0.5 * grad_b_out\n",
    "\n",
    "print(w_out)\n",
    "print('\\nBias\\n', b_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cflex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
