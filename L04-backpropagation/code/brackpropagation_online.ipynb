{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RN Backprogation online - step to step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo la arquitectura de la RN esta conformada por la capa de entrada $(in)$, una capa oculta $(h)$ y la capa de salida $(out)$. La arquitectura de la red se muestra en la siguiente imagen:\n",
    "\n",
    "<div>\n",
    "<img src=\"../img/rn_online_2-2-2.png\" align=\"center\" width=\"400\"/>\n",
    "<div style=\"text-align: justify;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen anterior tiene una capa de entrada $i$, una capa oculta $h$ y una capa de salida $k$. Las unidades en la capa oculta están completamente conectada a la capa de entrada y a la capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar la arquitectura de la red se utiliza una matriz  $W^{(h)}$ de dimensiones $i \\times j$ que represente el conjunto de pesos entre dos capas, donde j es el número de neuronas de la capa oculta. Por ejemplo, para los pesos entre la capa de entrada y la capa oculta, de la imagen anterioro se utiliza la matriz:\n",
    "\n",
    "$ W^{(h)} = \\begin{bmatrix} 0.15 & 0.25 \\\\ 0.20 & 0.30 \\end{bmatrix}$\n",
    "\n",
    "$ b^{(h)} = \\begin{bmatrix} 0.35 & 0.35  \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h = np.array([[0.15, 0.25],\n",
    "                [0.20, 0.30]])\n",
    "                \n",
    "b_h = np.array([0.35, 0.35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar los pesos entre la capa oculta y la capa de salida, de forma similar, se utiliza una matriz $W^{(out)}$ de dimensiones $j \\times k$ que representa los conexiones entre ambas capas:\n",
    "\n",
    "$ W^{(out)} = \\begin{bmatrix} 0.40 & 0.50 \\\\ 0.45 & 0.55  \\end{bmatrix}$\n",
    "\n",
    "$ b^{(out)} = \\begin{bmatrix}  0.60  & 0.60 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_out = np.array([[0.40, 0.50], \n",
    "                  [0.45, 0.55]])\n",
    "                  \n",
    "b_out = np.array([0.60, 0.60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender como funciona esto dentro del contexto del aprendizaje de un modelo de MLP, se resume el proceso de aprendizaje en tres pasos:\n",
    "\n",
    "1. Comenzando en la capa de entrada, se propaga los patrones (**feedforward**) de la muestra de entrenamiento hacia adelante de la red para generar una salida.\n",
    "2. Con la salida actual de la red, se calcula el error que se desea minimizar utilizando la función de costo.\n",
    "3. Con el algoritmo de **backpropagation** se propaga el error, al encontrar la derivada con respecto a cada peso en la red, para actualizar los pesos.\n",
    "\n",
    "Existen dos formas de realizar el proceso de aprendizaje:\n",
    "\n",
    "* **Online**, significa que por cada ejemplo de la muestra de entrenamiento, que se presenta a la red, se realiza un ajuste de pesos o mediante,\n",
    "* **Batch**, donde se presentan todos los ejemplos de la muestra de entrenamiento y después de calcular los errores se ajustan solo una vez los pesos de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "Para simplificar el proceso, solo utilizaremos un dato de entrenamiento:\n",
    "\n",
    "| $x_1$ | $x_2$ | $t_1$ | $t_2$|\n",
    "|---|---|---|---|\n",
    "| 0.05 | 0.1 | 0.01 | 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = np.array([0.05, 0.1])\n",
    "target = np.array([0.01, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conocer la salida de la RN se propagan los patrones de la muestra de entrenamiento hacia adelante de la red, a partir de la capa de entrada.\n",
    "\n",
    "### Activación de la capa de entrada\n",
    "\n",
    "Para calcular la activación de las neuronas de la capa de entrada $a_i^{(in)}$ se consideran como tal las mismas entradas, es decir, el mismo patrón que se presenta a la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salida de las neronas de la capa de pattern[0]\n",
    "a_in_i1 = pattern[0]\n",
    "a_in_i2 = pattern[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activación de la capa oculta\n",
    "\n",
    "La activación de las neuronas de la capa oculta $a_j^{(h)}$ se realiza mediante la suma de la propagación de la capa anterior (capa de entrada) hacia la neurona de la capa de salida, que se desea calcular la entrada. \n",
    "\n",
    "Es decir, el cálculo de la propagación de la neurona $a_j^{(h)}$ se determina mediante $z_j^{(h)}$:\n",
    "\n",
    "$z_j^{(h)} = a_0^{(in)}w_{0,j}^{(h)}+a_1^{(in)}w_{1,j}^{(h)}+ \\cdots +a_i^{(in)}w_{i,j}^{(h)} = \\sum_{i} a_i^{(in)}w_{i,j}$.\n",
    "\n",
    "Para calcular la salida de las neurona de la capa oculta, se utiliza la función sigmoidea: $\\phi\\big(z\\big) = \\frac{1}{1 + e^{-z}}$, entoces la salida de las neurona $a_j^{(h)}$ se calcula mediante:  $a_j^{(h)} = \\phi\\big(z_j^{(h)}\\big)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada h1 =  0.3775\n",
      "Entrada h2 =  0.3925\n",
      "Salida h1 =  0.5932699921071872\n",
      "Salida h2 =  0.596884378259767\n"
     ]
    }
   ],
   "source": [
    "# Entrada de las neuronas de la capa oculta\n",
    "z_h_h1 = round(a_in_i1 * w_h[0][0] + a_in_i2 * w_h[1][0] + 1 * b_h[0], 5)\n",
    "z_h_h2 = round(a_in_i1 * w_h[0][1] + a_in_i2 * w_h[1][1] + 1 * b_h[1], 5)\n",
    "\n",
    "print(\"Entrada h1 = \", z_h_h1)\n",
    "print(\"Entrada h2 = \", z_h_h2)\n",
    "\n",
    "# Salida de las neuronas de la capa oculta\n",
    "a_h_h1 = 1. / (1. + math.exp(- z_h_h1))\n",
    "a_h_h2 = 1. / (1. + math.exp(- z_h_h2))\n",
    "\n",
    "print(\"Salida h1 = \", a_h_h1)\n",
    "print(\"Salida h2 = \", a_h_h2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activación de la capa de salida\n",
    "\n",
    "Para la activación de las neuronas de la capa de salida $a_k^{(out)}$, el cálculo de las entradas y salidas de cada una de las neuronas se realiza de la misma forma que en la capa oculta. Ejemplo, para calcular la activación $a_k^{(out)}$, se considera el valor de entrada $z_k^{(out)}$:\n",
    "\n",
    "$z_k^{(out)} = a_0^{(h)}w_{0,k}^{(out)}+a_1^{(h)}w_{1,k}^{(out)}+ \\cdots +a_j^{(h)}w_{j,k}^{(out)}$\n",
    "\n",
    "Para calcular la salida de las neurona de la capa de salida, nuevamente se utiliza la función sigmoidea: $\\phi\\big(z\\big) = \\frac{1}{1 + e^{-z}}$, entoces la salida de las neurona $a_k^{(out)}$ se calcula mediante:  $a_k^{(out)} = \\phi\\big(z_k^{(out)}\\big)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada o1 =  1.10590596705977\n",
      "Entrada o2 =  1.2249214040964653\n",
      "Salida o1 =  0.7513650695523157\n",
      "Salida o2 =  0.7729284653214625\n"
     ]
    }
   ],
   "source": [
    "# Entrada a las neuronas de la capa oculta\n",
    "z_out_o1 = a_h_h1 * w_out[0][0] + a_h_h2 * w_out[1][0] + 1 * b_out[0]\n",
    "z_out_o2 = a_h_h1 * w_out[0][1] + a_h_h2 * w_out[1][1] + 1 * b_out[1]\n",
    "\n",
    "print(\"Entrada o1 = \", z_out_o1)\n",
    "print(\"Entrada o2 = \", z_out_o2)\n",
    "\n",
    "# Salida de las neuronas de la capa oculta\n",
    "a_out_o1 = 1 / (1 + math.exp(- z_out_o1))\n",
    "a_out_o2 = 1 / (1 + math.exp(- z_out_o2))\n",
    "\n",
    "print(\"Salida o1 = \", a_out_o1)\n",
    "print(\"Salida o2 = \", a_out_o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo del error de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error total\n",
    " \n",
    "Para calcular el error de la red se utiliza la función del Error Cuadrático Medio o Mean Square Error (MSE).\n",
    "\n",
    "$E_{total} = \\frac{1}{2} \\Sigma_{i=0}^m (target_k - a_k^{(out))})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error o1 =  0.27481\n",
      "Error o2 =  0.02356\n",
      "Error total =  0.29837\n"
     ]
    }
   ],
   "source": [
    "error_o1 = round((target[0] - a_out_o1)**2 * 0.5, 5)\n",
    "error_o2 = round((target[1] - a_out_o2)**2  * 0.5, 5)\n",
    "error_total = error_o1 + error_o2\n",
    "print(\"Error o1 = \", error_o1)\n",
    "print(\"Error o2 = \", error_o2)\n",
    "print(\"Error total = \", error_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidos los valores de activación de la capa de salida y el error total de la red se realiza el ajuste de los pesos, para aprender el patrón que ha sido presentado a la red.\n",
    "\n",
    "En este ejemplo se implementa el algoritmo de aprendizaje de backprogation con **gradiente descendiente** o **Gradient Descent (GD)** sobre **MSE**.\n",
    "\n",
    "El objetivo en el aprendizaje es ajustar los pesos de la red para reducir el error general de la red ante el patron presentado, a partir de la siguiente ecuación:\n",
    "\n",
    "$\\Delta W \\eta - \\frac{\\partial E}{\\partial W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de los pesos de la capa de salida\n",
    "\n",
    "Consideremos un peso en particular de la capa de salida\n",
    "\n",
    "$\\Delta w_{h_j, o_k} \\alpha - \\frac{\\partial E}{\\partial w_{h_j,o_k}}$\n",
    "\n",
    "Es necesario considerar que el error no es directamente una función de un error. Por que la ecuación se expande a:\n",
    "\n",
    "$\\Delta w_{h_j,o_kj} = - \\eta\n",
    "\\frac{\\partial E}{\\partial out_{o_k}} * \n",
    "\\frac{\\partial out_{o_k}}{\\partial net_{o_k}} * \n",
    "\\frac{\\partial net_{o_k}}{\\partial w_{h_j,h_k}}$\n",
    "\n",
    "### Regla de cambio de pesos para un peso entre la capa oculta y de salida\n",
    "\n",
    "$\\Delta w_{h_j,o_k} = - \\eta  (t_{o_k} - a_k^{(out)}) * a_k^{(out)} * ( 1 - a_k^{(out)}) * a_j^{(h)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derivada del error con respecto a la activación: $- (t_{k} - a_k^{(out)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivadad del error total o1 = 0.74137\n",
      "Derivadad del error total o2 = -0.21707\n"
     ]
    }
   ],
   "source": [
    "derivate_error_o1 = round(-(target[0] - a_out_o1), 5)\n",
    "derivate_error_o2 = round(-(target[1] - a_out_o2), 5)\n",
    "\n",
    "print(\"Derivadad del error total o1 =\" , derivate_error_o1)\n",
    "print(\"Derivadad del error total o2 =\" , derivate_error_o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Derivada de la activación con respecto a la entrada: $a_k^{(out)} * ( 1 - a_k^{(out)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivate of the logistic o1 = 0.18682\n",
      "Derivate of the logistic o1 = 0.17551\n"
     ]
    }
   ],
   "source": [
    "\n",
    "derivate_sigmoid_o1 = round( a_out_o1 * (1 - a_out_o1), 5)\n",
    "derivate_sigmoid_o2 = round( a_out_o2 * (1 - a_out_o2), 5)\n",
    "\n",
    "print(\"Derivate of the logistic o1 =\", derivate_sigmoid_o1)\n",
    "print(\"Derivate of the logistic o1 =\", derivate_sigmoid_o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Derivada de la entrada con respecto al peso: $a_j^{(h)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de h1 0.5932699921071872\n",
      "Salida de h2 0.596884378259767\n"
     ]
    }
   ],
   "source": [
    "print(\"Salida de h1\", a_h_h1)\n",
    "print(\"Salida de h2\", a_h_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se considera que $\\delta_k^{(out)} = (t_k - a_k^{(out)}) * a_k^{(out)} * (1- a_k^{(out)}) $￼, entonces la regla es muy similar a la del Perceptrón.\n",
    "\n",
    "$\\Delta w_{h_j, o_k} = - \\eta \\delta_{o_k} * a_j^{(h)}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta o1 =  0.1385\n",
      "delta o2 =  -0.0381\n"
     ]
    }
   ],
   "source": [
    "delta_o1 = round(-(target[0] - a_out_o1) * a_out_o1 * (1 - a_out_o1), 5)\n",
    "delta_o2 = round(-(target[1] - a_out_o2) * a_out_o2 * (1 - a_out_o2), 5)\n",
    "\n",
    "print(\"delta o1 = \", delta_o1)\n",
    "print(\"delta o2 = \", delta_o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de pesos de la capa oculta\n",
    "\n",
    "La manera de cambiar los pesos de entre la capa de entrada y la oculta depende del error de todos los nodos, esta conexión lleva a plantaer la siguiente ecuación:\n",
    "\n",
    "$\\Delta w_{ji} \\, \\alpha -\n",
    "\\big[ \n",
    "\\sum_k \n",
    "\\frac{\\partial E_{total}}{\\partial out_{o_k}} \\frac{\\partial out_{o_k}}{\\partial net_{o_k}} \\frac{\\partial net_{o_k}}{\\partial out_{h_j}}\n",
    "\\big] \n",
    "\\frac{\\partial out_{h_j}}{\\partial net_{h_j}} \n",
    "\\frac{\\partial net_{h_j}}{\\partial w_{h_j,i_i}}$\n",
    "\n",
    "$\\Delta w_{ji} = \\eta \n",
    "\\big[ \n",
    "\\sum_k \n",
    "(t_{o_k} - a_k^{(out)}) * a_k^{(out)} * (1- a_k^{(out)}) w_{h_j, o_k}\n",
    "\\big] * \n",
    "a_j^{(h)} * (1 - a_j^{(h)}) *\n",
    "a_i^{(in)}$\n",
    "\n",
    "$\\Delta w_{ji} = \\eta \n",
    "\\big[ \n",
    "\\sum_k \n",
    "\\delta_k^{(out)} w_{h_j, o_k}\n",
    "\\big] * \n",
    "a_j^{(h)} * (1 - a_j^{(h)}) *\n",
    "a_i^{(in)}$\n",
    "￼￼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05540109736000001\n",
      "-0.019048977850000003\n",
      "0.06232623453000001\n",
      "-0.020953875635000004\n",
      "Derivada del Error  total de h1 =  0.03635211951000001\n",
      "Derivada del Error  total de h2 =  0.041372358895\n",
      "derivate of the logistic h1 =  0.24130070857232525\n",
      "derivate of the logistic h2 =  0.2406134172492184\n",
      "Salida de i1 =  0.05\n",
      "Salida de i2 =  0.1\n",
      "Delta h1 0.008771792195868851\n",
      "Delta h2 0.009954744653387047\n"
     ]
    }
   ],
   "source": [
    "derivate_error_h1_o1 = derivate_error_o1 * derivate_sigmoid_o1 * w_out[0][0]\n",
    "derivate_error_h1_o2 = derivate_error_o2 * derivate_sigmoid_o2 * w_out[0][1]\n",
    "print(derivate_error_h1_o1)\n",
    "print(derivate_error_h1_o2)\n",
    "\n",
    "derivate_error_h2_o1 = derivate_error_o1 * derivate_sigmoid_o1 * w_out[1][0]\n",
    "derivate_error_h2_o2 = derivate_error_o2 * derivate_sigmoid_o2 * w_out[1][1]\n",
    "print(derivate_error_h2_o1)\n",
    "print(derivate_error_h2_o2)\n",
    "\n",
    "\n",
    "error_h1 = derivate_error_h1_o1 + derivate_error_h1_o2\n",
    "print(\"Derivada del Error  total de h1 = \", error_h1 )\n",
    "\n",
    "error_h2 = derivate_error_h2_o1 + derivate_error_h2_o2\n",
    "print(\"Derivada del Error  total de h2 = \", error_h2 )\n",
    "\n",
    "\n",
    "derivate_sigmoid_h1 = a_h_h1 * (1 - a_h_h1)\n",
    "print(\"derivate of the logistic h1 = \" , derivate_sigmoid_h1)\n",
    "\n",
    "derivate_sigmoid_h2 = a_h_h2 * (1 - a_h_h2)\n",
    "print(\"derivate of the logistic h2 = \" , derivate_sigmoid_h2)\n",
    "\n",
    "# Ahora se calcula la derivada parcial de la entrad de h1 con respecto a w1 que es igual a la entrada i\n",
    "print(\"Salida de i1 = \", a_in_i1)\n",
    "print(\"Salida de i2 = \", a_in_i2)\n",
    "\n",
    "delta_h1 = error_h1 * derivate_sigmoid_h1 #* a_in_i1\n",
    "delta_h2 = error_h2 * derivate_sigmoid_h2 #* a_in_i2\n",
    "\n",
    "print(\"Delta h1\", delta_h1)\n",
    "print(\"Delta h2\", delta_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que los valores $\\delta$ de cada capa han sido obtenidos se realizar el cambio de pesos de todas las capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla de cambio de pesos para un peso entre la capa oculta y de salida\n",
    "\n",
    "Cambio de pesos en la capa de salida mediante la regla:\n",
    "\n",
    "$\\Delta w_{h_j, o_k} = - \\eta \\delta_{o_k} * a_j^{(h)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35891605 0.51137065]\n",
      " [0.40891605 0.56137065]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "# Pesos conectados a la neurona o1\n",
    "w_out[0][0] = w_out[0][0] - eta * delta_o1 * a_h_h1\n",
    "w_out[0][1] = w_out[0][1] - eta * delta_o2 * a_h_h2\n",
    "# Pesos conectados a la neurona o2\n",
    "w_out[1][0] = w_out[1][0] - eta * delta_o1 * a_h_h1\n",
    "w_out[1][1] = w_out[1][1] - eta * delta_o2 * a_h_h2\n",
    "\n",
    "print(w_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53075 0.61905]\n"
     ]
    }
   ],
   "source": [
    "b_out[0] = b_out[0] - eta * delta_o1\n",
    "b_out[1] = b_out[1] - eta * delta_o2\n",
    "\n",
    "print(b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regla de cambio de pesos entre la capa de entrada y oculta\n",
    "\n",
    "Si consideramos que $\\delta_{h_j} = \\big[ \\sum_k w_{o_k,h_j} \\big ] * out_{h_j} * (1- out_{h_j}) $, entonces la regla es muy similar a la anterior:\n",
    "\n",
    "$\\Delta w_{ij} = - \\eta \\delta_{h_j} out_{i_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14978071 0.24975113]\n",
      " [0.19956141 0.29950226]]\n"
     ]
    }
   ],
   "source": [
    "w_h[0][0] = w_h[0][0] - eta * delta_h1 * a_in_i1\n",
    "w_h[1][0] = w_h[1][0] - eta * delta_h1 * a_in_i2\n",
    "w_h[0][1] = w_h[0][1] - eta * delta_h2 * a_in_i1\n",
    "w_h[1][1] = w_h[1][1] - eta * delta_h2 * a_in_i2\n",
    "\n",
    "print(w_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53075 0.61905]\n"
     ]
    }
   ],
   "source": [
    "b_h[0] = b_h[0] - eta * delta_h1\n",
    "b_h[1] = b_h[1] - eta * delta_h2\n",
    "\n",
    "print(b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Este proceso se repite con cada ejemplo de la muestra de entrenamiento (época). \n",
    "\n",
    "* Al finalizar la época se reporta el error general de la red ante la muestra de entrenamiento.\n",
    "\n",
    "* En cada época se espera que el error sea reducido como indicador de aprendizaje de la red."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "395a4e92152996666fb2f9ccdfaa123fbee5cdfd349ce9639909b685c816356c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
